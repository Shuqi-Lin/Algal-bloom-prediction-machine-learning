{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import random\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "pd.set_option('display.max_columns', 100)  # or 1000\n",
    "pd.set_option('display.max_rows', 100)  # or 1000\n",
    "import time\n",
    "from numpy import concatenate\n",
    "Scaler = MinMaxScaler()\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing dataset(Daily data)\n",
    "def read_daily_df(test_time,features,file): # test_time = pd.Timestamp(yyyy,mm,dd)\n",
    "    Lake_HydMet = pd.read_csv(file,header = 0,sep = '\\t',parse_dates = ['Date'])\n",
    "    Lake_HydMet = Lake_HydMet[features]\n",
    "    return Lake_HydMet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in, n_out, var_name,dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1] # number of variables\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [(var_name[j]+'(t-%d)' % (i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [(var_name[j]+'(t)') for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [(var_name[j]+'(t+%d)' % (i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "def load_dataset(df,var_name):\n",
    "    values = df[var_name].values\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reframe dataset\n",
    "def reframe(values,hyperparameters,var_names):\n",
    "    reframed = series_to_supervised(values, hyperparameters['time_steps'], hyperparameters['n_out'],var_names)\n",
    "    reframed = reframed.iloc[hyperparameters['time_steps']:]\n",
    "    drop_col =[]\n",
    "    n_var = len(var_names)\n",
    "    for i in range(1,hyperparameters['time_steps']+1):\n",
    "        drop_col += [n_var*i-1]\n",
    "    reframed.drop(reframed.iloc[:,drop_col],axis=1,inplace = True)\n",
    "    return reframed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_dataset(data_X,data_y):\n",
    "    index = []\n",
    "    y = []\n",
    "    for i in range(len(data_y)):\n",
    "        if ~np.isnan(data_y[i]):\n",
    "            index.append(i)\n",
    "            y.append(data_y[i])\n",
    "    X = np.stack(data_X[index,:,:])\n",
    "    y = np.array(y)\n",
    "    return index,X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lstm(train_X,train_y,n_batch,nb_epoch,n_neuros,dropout,verbose):\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neuros,  return_sequences = True,\n",
    "              input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(n_neuros, return_sequences = True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(n_neuros))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_X,train_y,epochs =nb_epoch,batch_size = n_batch,verbose = verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(df,y,yhat,yhat_ts,n_date,time_steps,index,rmse,r2,nutrient,test_time,ylim):\n",
    "    # Observation time\n",
    "    time = df['Date'].iloc[n_date+time_steps:].reset_index()['Date'].iloc[index] \n",
    "    # Direct comparison of observation and prediction [yhat] (data point to data point)\n",
    "    pred = pd.DataFrame(concatenate((yhat.reshape(yhat.shape[0],1),y.reshape(y.shape[0],1)), axis=1),\n",
    "                        index = time)\n",
    "    pred.columns = ['Prediction','True value']\n",
    "    # Extract the continuous timeseries from input dataset\n",
    "    time_ts = df['Date'].iloc[n_date+time_steps:]\n",
    "    # The continuous prediction yhat_ts \n",
    "    pred_ts = pd.DataFrame(yhat_ts,index = time_ts,columns = ['Prediction'])\n",
    "    # Compute the 7d rolling mean of the timeseries predction\n",
    "    pred_ts['Prediction_7d'] = pred_ts['Prediction'].rolling(7,min_periods = 1).mean()\n",
    "    # Create a continous timeseries without winter gap\n",
    "    Date = pd.DataFrame(pd.date_range(start = time_ts.iloc[0],\n",
    "                                  end = time_ts.iloc[-1]),\n",
    "                    columns = ['Date'])\n",
    "    pred_ts_gap = Date.merge(pred_ts,how = 'left',on = 'Date')\n",
    "    f1,ax1 = plt.subplots(1,2,figsize = (18,6),gridspec_kw={'width_ratios': [2, 1]})\n",
    "    pred_ts_gap.plot(x = 'Date',y = ['Prediction','Prediction_7d'],\n",
    "                     style = {'Prediction':'b-','Prediction_7d':'k-'},\n",
    "                     ax = ax1[0])\n",
    "    pred.plot(y = 'True value',style='ro',alpha = 0.7,ms = 7,ax = ax1[0])\n",
    "    ax1[0].set_ylabel(nutrient)\n",
    "    ax1[0].set_xlim((test_time[0],test_time[1]))\n",
    "    ax1[0].set_ylim(ylim)\n",
    "    ax1[0].text(0.7, 0.9, 'RMSE:{}'.format(round(rmse,2)), \n",
    "            horizontalalignment='center',verticalalignment='center', \n",
    "            transform=ax1[0].transAxes,fontsize='x-large')\n",
    "    pred.plot(x = 'True value', y = 'Prediction',kind = 'scatter',s = 20,c = 'blue',ax = ax1[1])\n",
    "    ax1[1].plot(pred['True value'],pred['True value'],lw  =1.5,color = 'black')\n",
    "    ax1[1].text(0.5, 0.8, 'R2:{}'.format(round(r2,2)), \n",
    "                horizontalalignment='center',verticalalignment='center', \n",
    "                transform=ax1[1].transAxes,fontsize='x-large')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ts(df,nutrient,model,hyperparameters,values):\n",
    "    # add the predictive values into dataset\n",
    "    value_X, value_y = values[:, :-1], values[:, -1]\n",
    "    value_X = value_X.reshape((value_X.shape[0], hyperparameters['time_steps']+1, int(value_X.shape[1]/(hyperparameters['time_steps']+1))))\n",
    "    y_pred = Scaler.inverse_transform(model.predict(value_X,batch_size = hyperparameters['n_batch']))    \n",
    "    df[nutrient].iloc[hyperparameters['time_steps']:]=y_pred[:,0]\n",
    "    df[nutrient].fillna(method = 'backfill',inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(train,test,time_steps):\n",
    "    # split into input and outputs\n",
    "    train_X, train_y = train[:, :-1], train[:, -1]\n",
    "    test_X, test_y = test[:, :-1], test[:, -1]\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], time_steps+1, int(train_X.shape[1]/(time_steps+1))))\n",
    "    test_X = test_X.reshape((test_X.shape[0], time_steps+1, int(test_X.shape[1]/(time_steps+1))))\n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    print('number of input timesteps: {}'.format(train_X.shape[1]))\n",
    "    print('number of features: {}'.format(train_X.shape[2]))\n",
    "    return train_X, train_y,test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure all data is float\n",
    "def predict_lstm(df,values,var_name,nutrient,test_time,hyperparameters,ylim):\n",
    "    n_date = df[df['Date']<test_time[0]]['Date'].count()\n",
    "    train = values[:n_date, :]\n",
    "    test = values[n_date:, :]\n",
    "    train_X, train_y,test_X, test_y = split_dataset(train,test,hyperparameters['time_steps'])\n",
    "    # fit the lstm model\n",
    "    index,X,y = sparse_dataset(train_X,train_y) # stack the timeseries input together to create a 2D training input X, and a 1D lable y\n",
    "    y_scaled = Scaler.fit_transform(y.reshape(-1,1))\n",
    "    print('number of samples: {}'.format(len(index)))\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=101) # 5-fold cross validation\n",
    "    RMSE = []\n",
    "    R2 = []\n",
    "    # fit the lstm model \n",
    "    for train_index, test_index in kf.split(X,y_scaled):\n",
    "        #print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y_scaled[train_index], y_scaled[test_index]\n",
    "        model = fit_lstm(X_train,y_train,hyperparameters['n_batch'],hyperparameters['nb_epoch'],\n",
    "                         hyperparameters['n_neuros'],hyperparameters['dropout'],hyperparameters['verbose'])\n",
    "        yhat = Scaler.inverse_transform(model.predict(X_test,batch_size = hyperparameters['n_batch']))\n",
    "        y_test = Scaler.inverse_transform(y_test)\n",
    "        rmse = sqrt(mean_squared_error(y_test, yhat))\n",
    "        r2 =  r2_score(y_test, yhat)\n",
    "        RMSE.append(rmse) \n",
    "        R2.append(r2)\n",
    "        #print('Training RMSE: %.2f' %rmse)       \n",
    "    # make a prediction\n",
    "    index,X,y = sparse_dataset(test_X,test_y) # index is the time series\n",
    "    yhat = Scaler.inverse_transform(model.predict(X,batch_size = hyperparameters['n_batch']))\n",
    "    rmse = sqrt(mean_squared_error(y, yhat))\n",
    "    r2 = r2_score(y, yhat)\n",
    "    print('Test RMSE: %.2f' % rmse)\n",
    "    print('Test R2: %.2f' %r2)\n",
    "    # make a prediction for the whole timeseries\n",
    "    yhat_ts = Scaler.inverse_transform(model.predict(test_X,batch_size = hyperparameters['n_batch']))\n",
    "    #figure = plot_comparison(df,y,yhat,yhat_ts,n_date,hyperparameters['time_steps'],index,rmse,r2,nutrient,test_time,ylim)\n",
    "    return y,yhat,yhat_ts,n_date,index,rmse,r2 #model,RMSE,R2,figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "index,X,y = sparse_dataset(train_X,train_y) # index is the time series\n",
    "y_scaled = Scaler.fit_transform(y.reshape(-1,1))\n",
    "model = fit_lstm(X,y_scaled,hyperparameters['n_batch'],hyperparameters['nb_epoch'],\n",
    "                 hyperparameters['n_neuros'],hyperparameters['dropout'],hyperparameters['verbose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "index,X,y = sparse_dataset(test_X,test_y)\n",
    "yhat = Scaler.inverse_transform(model.predict(X,batch_size = hyperparameters['n_batch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load training data (containing nutrient observations)\n",
      "Lake name in short:LV\n",
      "Winter data or not? (Y/N)N\n"
     ]
    }
   ],
   "source": [
    "## Load training dataset\n",
    "# Make sure you are in the main folder('..\\Algal-bloom-prediction-machine-learning')\n",
    "os.chdir('Trainning data')\n",
    "print('Load training data (containing nutrient observations)')\n",
    "lakename = input('Lake name in short:')\n",
    "winter = input('Winter data or not? (Y/N)')\n",
    "if winter =='Y':\n",
    "    all_df = pd.read_csv(lakename+'_Observation_df.csv',sep = '\\t',parse_dates = ['Date'])\n",
    "else:\n",
    "    all_df = pd.read_csv(lakename+'_Observation_df_nowinter.csv',sep = '\\t',parse_dates = ['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 259 entries, 0 to 258\n",
      "Data columns (total 26 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   Date              259 non-null    datetime64[ns]\n",
      " 1   chl-a             255 non-null    float64       \n",
      " 2   NH4N              224 non-null    float64       \n",
      " 3   NO3N              232 non-null    float64       \n",
      " 4   PO4P              253 non-null    float64       \n",
      " 5   SiO2              244 non-null    float64       \n",
      " 6   TOC               151 non-null    float64       \n",
      " 7   TOTN              245 non-null    float64       \n",
      " 8   TOTP              253 non-null    float64       \n",
      " 9   O2                206 non-null    float64       \n",
      " 10  AirP              246 non-null    float64       \n",
      " 11  AirT              246 non-null    float64       \n",
      " 12  RelHum            246 non-null    float64       \n",
      " 13  SWR               246 non-null    float64       \n",
      " 14  CC                246 non-null    float64       \n",
      " 15  Prec              246 non-null    float64       \n",
      " 16  U                 246 non-null    float64       \n",
      " 17  delT              246 non-null    float64       \n",
      " 18  inflow(m3/s)      246 non-null    float64       \n",
      " 19  outflow(m3/s)     246 non-null    float64       \n",
      " 20  Ice_d             246 non-null    float64       \n",
      " 21  days from iceoff  246 non-null    float64       \n",
      " 22  W                 200 non-null    float64       \n",
      " 23  MLD               246 non-null    float64       \n",
      " 24  thermD            246 non-null    float64       \n",
      " 25  MM                259 non-null    int64         \n",
      "dtypes: datetime64[ns](1), float64(24), int64(1)\n",
      "memory usage: 52.7 KB\n",
      "None\n",
      "Number of nutrients: (including Chl)9\n",
      "Nutrients: O2\n",
      "Nutrients: NH4N\n",
      "Nutrients: NO3N\n",
      "Nutrients: PO4P\n",
      "Nutrients: SiO2\n",
      "Nutrients: TOC\n",
      "Nutrients: TOTP\n",
      "Nutrients: TOTN\n",
      "Nutrients: chl-a\n",
      "['Date', 'O2', 'NH4N', 'NO3N', 'PO4P', 'SiO2', 'TOC', 'TOTP', 'TOTN', 'chl-a']\n"
     ]
    }
   ],
   "source": [
    "# Create the daily df with Nan in nutrients columns'\n",
    "print(all_df.info())\n",
    "n_Nut = int(input('Number of nutrients: (including Chl)'))\n",
    "Nut_f = ['Date']\n",
    "for i in range(n_Nut):\n",
    "    f = input('Nutrients: ')\n",
    "    Nut_f.append(f)\n",
    "print(Nut_f)\n",
    "Nut = all_df[Nut_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features used for basic training: 15\n",
      "Feature:AirP\n",
      "Feature:AirT\n",
      "Feature:U\n",
      "Feature:RelHum\n",
      "Feature:SWR\n",
      "Feature:CC\n",
      "Feature:Prec\n",
      "Feature:delT\n",
      "Feature:inflow(m3/s)\n",
      "Feature:outflow(m3/s)\n",
      "Feature:Ice_d\n",
      "Feature:days from iceoff\n",
      "Feature:W\n",
      "Feature:MLD\n",
      "Feature:thermD\n",
      "['Date', 'AirP', 'AirT', 'U', 'RelHum', 'SWR', 'CC', 'Prec', 'delT', 'inflow(m3/s)', 'outflow(m3/s)', 'Ice_d', 'days from iceoff', 'W', 'MLD', 'thermD']\n"
     ]
    }
   ],
   "source": [
    "n_feature = int(input('Number of features used for basic training: '))\n",
    "features = ['Date']\n",
    "for i in range(n_feature):\n",
    "    f = input('Feature:')\n",
    "    features.append(f)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016]\n",
      "test starts in year:2013\n",
      "test ends in year:2017\n"
     ]
    }
   ],
   "source": [
    "# date to seperate training and test sets\n",
    "YY=all_df['Date'].apply(lambda d:d.year).unique()\n",
    "print(YY)\n",
    "test_time = [pd.Timestamp(int(input('test starts in year:')),1,1),pd.Timestamp(int(input('test ends in year:')),1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read daily observation data\n",
    "if winter =='Y':\n",
    "    file = lakename+'_Daily_Observation_df.csv'\n",
    "else:\n",
    "    file = lakename+'_Daily_Observation_df_nowinter.csv'\n",
    "Daily_df = read_daily_df(test_time,features,file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters\n",
    "hyperparameters = {'n_batch':10,'nb_epoch':100,'n_neuros':100,'dropout':0.1,'time_steps':7,'n_out':1,'verbose':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nutrient: NO3N\n"
     ]
    }
   ],
   "source": [
    "# predict nutreint 1\n",
    "nutrient = input('Nutrient: ')\n",
    "# If add the features from process-based model, add features 'MLD','W','thermD'\n",
    "features.pop(0) # Remove 'Date'\n",
    "features.append(nutrient)\n",
    "Nut_memory = Daily_df.merge(pd.concat([Nut['Date'],Nut[nutrient]],axis = 1),how = 'left',on = 'Date')\n",
    "df = Nut_memory.dropna()\n",
    "values = load_dataset(Nut_memory,features) #values = values.astype('float32')\n",
    "# frame as supervised learning\n",
    "reframed = reframe(values,hyperparameters,features)\n",
    "values = reframed.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Nut_memory.dropna()\n",
    "values = load_dataset(df,features) #values = values.astype('float32')\n",
    "# frame as supervised learning\n",
    "reframed = reframe(values,hyperparameters,features)\n",
    "values = reframed.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 8, 15) (149,) (26, 8, 15) (26,)\n",
      "number of input timesteps: 8\n",
      "number of features: 15\n",
      "number of samples: 149\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13080/1224667961.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Nut_model,RMSE,R2,figure =\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0myhat_ts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnutrient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_time\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mylim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# 5-fold cross validation was used to estimate the model performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#print(\"Training dataset RMSE %.2f (+/- %.2f)\" % (np.mean(RMSE), np.std(RMSE)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13080/2752414479.py\u001b[0m in \u001b[0;36mpredict_lstm\u001b[1;34m(df, values, var_name, nutrient, test_time, hyperparameters, ylim)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_scaled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_scaled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         model = fit_lstm(X_train,y_train,hyperparameters['n_batch'],hyperparameters['nb_epoch'],\n\u001b[0m\u001b[0;32m     20\u001b[0m                          hyperparameters['n_neuros'],hyperparameters['dropout'],hyperparameters['verbose'])\n\u001b[0;32m     21\u001b[0m         \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_batch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13080/4097359698.py\u001b[0m in \u001b[0;36mfit_lstm\u001b[1;34m(train_X, train_y, n_batch, nb_epoch, n_neuros, dropout, verbose)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# fit network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\documents\\d\\tensorflow_virtual_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\documents\\d\\tensorflow_virtual_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\documents\\d\\tensorflow_virtual_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\documents\\d\\tensorflow_virtual_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\documents\\d\\tensorflow_virtual_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\documents\\d\\tensorflow_virtual_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\documents\\d\\tensorflow_virtual_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\documents\\d\\tensorflow_virtual_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\documents\\d\\tensorflow_virtual_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ylim = (0,all_df[nutrient].max())\n",
    "start_time = time.time()\n",
    "#Nut_model,RMSE,R2,figure = \n",
    "yhat_ts = predict_lstm(df,values,features,nutrient,test_time,hyperparameters,ylim)\n",
    "# 5-fold cross validation was used to estimate the model performance\n",
    "#print(\"Training dataset RMSE %.2f (+/- %.2f)\" % (np.mean(RMSE), np.std(RMSE)))\n",
    "#print(\"Training dataset R2 %.2f (+/- %.2f)\" % (np.mean(R2), np.std(R2)))\n",
    "#print('Model takes '+str(round((time.time()-start_time)/60))+' min to run')\n",
    "#figure.savefig(lakename+'LSTM_'+nutrient+'.png',dpi = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-131-2b600ecf9f32>\", line 2, in <module>\n",
      "    Nut_memory = predict_ts(Nut_memory,nutrient,Nut_model,hyperparameters,values)\n",
      "  File \"<ipython-input-10-934583d208c7>\", line 6, in predict_ts\n",
      "    df[nutrient].iloc[hyperparameters['time_steps']:]=y_pred[:,0]\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\", line 205, in __setitem__\n",
      "    self._setitem_with_indexer(indexer, value)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\", line 593, in _setitem_with_indexer\n",
      "    self.obj._data = self.obj._data.setitem(indexer=indexer, value=value)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 560, in setitem\n",
      "    return self.apply(\"setitem\", **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 438, in apply\n",
      "    applied = getattr(b, f)(**kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\", line 914, in setitem\n",
      "    check_setitem_lengths(indexer, value, values)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexers.py\", line 110, in check_setitem_lengths\n",
      "    \"cannot set using a slice indexer with a \"\n",
      "ValueError: cannot set using a slice indexer with a different length than the value\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ValueError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot set using a slice indexer with a different length than the value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# use the trained model to interplate the whole timeseries\n",
    "Nut_memory = predict_ts(Nut_memory,nutrient,Nut_model,hyperparameters,values)\n",
    "Nut_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict nutreint 2\n",
    "nutrient = input('Nutrient: ')\n",
    "\n",
    "delet = input('Delete features? (Y/N)')\n",
    "while delet=='Y':\n",
    "    print(features)\n",
    "    idx = int(input('Location of the feature:'))\n",
    "    features.pop(idx)\n",
    "    delet = input('Delete features? (Y/N)')\n",
    "add = input('Add features? (Y/N)')\n",
    "while add =='Y':\n",
    "    f = input('New feature:')\n",
    "    features.append(f)\n",
    "    print(features)\n",
    "    add = input('Add features? (Y/N)') \n",
    "\n",
    "features.append(nutrient)\n",
    "Nut_memory = Nut_memory.merge(pd.concat([Nut['Date'],Nut[nutrient]],axis = 1),how = 'left',on = 'Date')\n",
    "df = Nut_memory.dropna()\n",
    "df = pd.concat([Nut_memory['Date'],Nut_memory[features]],axis = 1).dropna()\n",
    "values = load_dataset(df,features) #values = values.astype('float32')\n",
    "# frame as supervised learning\n",
    "reframed = reframe(values,hyperparameters,features)\n",
    "values = reframed.values\n",
    "\n",
    "ylim = (0,all_df[nutrient].max())\n",
    "start_time = time.time()\n",
    "Nut_model,RMSE,R2,figure = predict_lstm(df,values,features,nutrient,test_time,hyperparameters,ylim)\n",
    "# 5-fold cross validation was used to estimate the model performance\n",
    "print(\"Training dataset RMSE %.2f (+/- %.2f)\" % (np.mean(RMSE), np.std(RMSE)))\n",
    "print(\"Training dataset R2 %.2f (+/- %.2f)\" % (np.mean(R2), np.std(R2)))\n",
    "print('Model takes '+str(round((time.time()-start_time)/60))+' min to run')\n",
    "figure.savefig('LSTM_NOX.png',dpi = 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
